{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with NLTK MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |#ID | #Gold Standard | #Found | #Correct | Precision | Recall | F-1 |\n",
    " |----|----------------|--------|----------|-----------|--------|-----|\n",
    " |1|3413|45|1|0.022222222222222223|0.0002929973630237328|0.000578368999421631|\n",
    " |2|3413|291|61|0.209621993127|0.0178728391444|0.0329373650108|\n",
    " |3|3413|1843|629|0.341291372762|0.184295341342|0.239345509893|\n",
    " |4|3413|1729|660|0.381723539618|0.193378259596|0.256709451575|\n",
    " |5|3413|2830|833|0.294346289753|0.244066803399|0.266858881948|\n",
    " |6|3413|1950|843|0.432307692308|0.246996777029|0.314376281932|\n",
    "|7|3413|2090|1009|0.482775119617|0.295634339291|0.366709067781|\n",
    "|8|3413|2217|1118|0.504285069914|0.327571051861|0.397158081705|\n",
    "|9|3413|2396|1338|0.558430717863|0.392030471726|0.460664486142|\n",
    "|10|3413|2355|1319|0.56008492569|0.386463521828|0.457350901526|\n",
    "|11|3413|2479|1432|0.577652279145|0.41957222385|0.486082824168|\n",
    "|12|3413|2601|1556|0.598231449443|0.455903896865|0.517459261723|\n",
    "|13|3372|2570|1508|0.586770428016|0.447212336892|0.507573207674|\n",
    "Training on 80% of data and 20% dev with NLTK MaxEnt\n",
    "\n",
    " 1. Feature : POS, Word Position\n",
    " 2. Feature : POS, Word Position, Word\n",
    " 3. Feature : POS, Word Position, Word, word Shape\n",
    " 4. Feature : POS, Previous POS, Word Position, Word, word Shape\n",
    " 5. Feature : current pos, previous pos, word position, current word, previous word, word Shape\n",
    " 6. Feature : current pos, previous pos, word position, current word, previous word, current word shape, previous word shape\n",
    " 7. Feature : {current, previous, next} pos, word position, {current, previous, next} word, {current, previous, next} word shape\n",
    " 8. Feature : {current, previous, next} pos, word position, {current, previous, next} word, {current, previous, next} word shape, current word len\n",
    " 9. Feature : {current, previous, next} {pos, word, word shape}, current word len, word position, suffix3, prefix3\n",
    " 10. Feature : {current, previous, next} {pos, word, word_shape, lemma},  word position, current word len, suffix3, prefix3\n",
    " 11. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len},  word position, suffix3, prefix3\n",
    " 12. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position\n",
    " 13. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position\n",
    "     - Random Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:16.592205Z",
     "start_time": "2017-11-24T08:13:16.572057Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:17.020588Z",
     "start_time": "2017-11-24T08:13:16.593737Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "from evalNER import eval\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk import MaxentClassifier\n",
    "from nltk.chunk import named_entity\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:17.059530Z",
     "start_time": "2017-11-24T08:13:17.025289Z"
    }
   },
   "outputs": [],
   "source": [
    "text = open(\"./gene-trainF17.txt\").read()\n",
    "lines = [ y.strip() for y in text.split(\"\\n\\n\")]\n",
    "raw_df = pd.DataFrame(lines, columns = [\"sentence\"])\n",
    "# np.random.seed(1234)\n",
    "msk = np.random.rand(len(raw_df)) < 0.8\n",
    "train_df = raw_df[msk]\n",
    "dev_df = raw_df[~msk]\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:17.071266Z",
     "start_time": "2017-11-24T08:13:17.060600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:17.386307Z",
     "start_time": "2017-11-24T08:13:17.072526Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"sentence_token\"] = df[\"sentence\"].apply(lambda x : [y.split(\"\\t\") for y in x.split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:29.842990Z",
     "start_time": "2017-11-24T08:13:17.389254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"tags\"] = df[\"sentence_token\"].apply(lambda x : [y[2] for y in x])\n",
    "df.loc[:, \"words\"] = df[\"sentence_token\"].apply(lambda x : [y[1] for y in x]) \n",
    "df.loc[:, \"pos\"] = df[\"words\"].apply(lambda x : [ y[1] for y in nltk.pos_tag(x)])\n",
    "df[\"pos_tags\"] = df[[\"pos\", \"tags\"]].apply(lambda x : zip(x[0],x[1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:13:40.907059Z",
     "start_time": "2017-11-24T08:13:29.847247Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(word, prev_word, next_word, pos_tag, prev_pos, next_pos, num):\n",
    "    feature = {}\n",
    "    \n",
    "    feature[\"index\"] = num\n",
    "    \n",
    "    prefix = lambda x,y : x[:y].lower() if x else None\n",
    "    \n",
    "    feature[\"prev_prefix3\"] = prefix(prev_word,3)\n",
    "    feature[\"curr_prefix3\"] = prefix(word,3)\n",
    "    feature[\"next_prefix3\"] = prefix(next_word,3)\n",
    "    \n",
    "    suffix = lambda x, y : x[-y:].lower() if x else None\n",
    "    \n",
    "    feature[\"prev_suffix3\"] = suffix(prev_word,3)\n",
    "    feature[\"curr_suffix3\"] = suffix(word,3)\n",
    "    feature[\"next_suffix3\"] = suffix(next_word,3)\n",
    "    \n",
    "    feature[\"prev_len\"] = len(prev_word) if prev_word else 0\n",
    "    feature[\"curr_len\"] = len(word)\n",
    "    feature[\"next_len\"] = len(next_word) if next_word else 0\n",
    "    \n",
    "    feature[\"prev_word\"] = prev_word\n",
    "    feature[\"curr_word\"] = word\n",
    "    feature[\"next_word\"] = next_word\n",
    "    \n",
    "    feature[\"prev_pos\"] = prev_pos\n",
    "    feature[\"curr_pos\"] = pos_tag\n",
    "    feature[\"next_pos\"] = next_pos\n",
    "    \n",
    "    feature[\"curr_shape\"] = named_entity.shape(word)\n",
    "    feature[\"prev_shape\"] = named_entity.shape(prev_word) if prev_word else None\n",
    "    feature[\"next_shape\"] = named_entity.shape(next_word) if next_word else None\n",
    "    \n",
    "    feature[\"curr_lemma\"] = stemmer.stem(word)\n",
    "    feature[\"prev_lemma\"] = stemmer.stem(prev_word) if prev_word else None\n",
    "    feature[\"next_lemma\"] = stemmer.stem(next_word) if next_word else None\n",
    "    \n",
    "    return feature\n",
    "\n",
    "features = []\n",
    "\n",
    "for words_pos_tags in zip(df[\"words\"].tolist(), df[\"pos_tags\"].tolist()):\n",
    "    prev_pos, prev_word = None, None\n",
    "    zipped_list = zip(words_pos_tags[0], words_pos_tags[1])\n",
    "    for num, word_pos_tag in enumerate(zipped_list, 1):\n",
    "        feature = tuple([extract_features(word_pos_tag[0], \n",
    "                                          prev_word,\n",
    "                                          zipped_list[num][0] if num < len(zipped_list) else None, \n",
    "                                          word_pos_tag[1][0], \n",
    "                                          prev_pos, \n",
    "                                          zipped_list[num][1][0] if num < len(zipped_list) else None,\n",
    "                                          num), word_pos_tag[-1][1]])\n",
    "        \n",
    "        prev_pos, prev_word = word_pos_tag[1][0], word_pos_tag[0]\n",
    "        features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:20:33.550832Z",
     "start_time": "2017-11-24T08:13:41.700641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception RuntimeError: 'generator ignored GeneratorExit' in <generator object find_file_iter at 0x7f443494ea50> ignored\n"
     ]
    }
   ],
   "source": [
    "memm_classifier = MaxentClassifier.train(features, \"megam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:20:36.666630Z",
     "start_time": "2017-11-24T08:20:33.552228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = dev_df.copy()\n",
    "test_df.loc[:, \"sentence_token\"] = test_df[\"sentence\"].apply(lambda x : [y.split(\"\\t\") for y in x.split(\"\\n\")])\n",
    "test_df.loc[:, \"tags\"] = test_df[\"sentence_token\"].apply(lambda x : [y[2] for y in x])\n",
    "test_df.loc[:, \"words\"] = test_df[\"sentence_token\"].apply(lambda x : [y[1] for y in x])\n",
    "test_df.loc[:, \"pos\"] = test_df[\"words\"].apply(lambda x :  [x[1] for x in nltk.pos_tag(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:20:43.913881Z",
     "start_time": "2017-11-24T08:20:36.667784Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_memm(pos, words):\n",
    "    features = []\n",
    "    prev_pos, prev_word = None, None\n",
    "    zipped_list = zip(words, pos)\n",
    "    for num, word_pos in enumerate(zipped_list, 1):\n",
    "        features.append(extract_features(word_pos[0], \n",
    "                                         prev_word,\n",
    "                                         zipped_list[num][0] if num < len(zipped_list) else None,\n",
    "                                         word_pos[1], \n",
    "                                         prev_pos,\n",
    "                                         zipped_list[num][1] if num < len(zipped_list) else None,\n",
    "                                         num))\n",
    "        prev_pos, prev_word = word_pos[1], word_pos[0]\n",
    "    return zip(words, memm_classifier.classify_many(features))\n",
    "\n",
    "test_df.loc[:, \"prediction\"] = test_df[[\"pos\", \"words\"]].apply(lambda x: predict_memm(x[0], x[1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T08:20:44.252914Z",
     "start_time": "2017-11-24T08:20:43.915265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3372, ' entities in gold standard.')\n",
      "(2570, ' total entities found.')\n",
      "(1508, ' of which were correct.')\n",
      "('Precision: ', 0.5867704280155642, 'Recall: ', 0.4472123368920522, 'F1-measure: ', 0.5075732076741839)\n",
      "|3372|2570|1508|0.586770428016|0.447212336892|0.507573207674|\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[:, \"temp1\"] = test_df[\"prediction\"].apply(lambda x : [str(i) + \"\\t\" + \"\\t\".join(y) for i,y in enumerate(x,1)])\n",
    "test_df.loc[:, \"temp1\"] = test_df[\"temp1\"].apply(lambda x : \"\\n\".join(x))\n",
    "\n",
    "predictions = \"\\n\\n\".join(test_df[\"temp1\"].tolist())\n",
    "gold_standard = \"\\n\\n\".join(test_df[\"sentence\"].tolist())\n",
    "eval(StringIO(gold_standard), StringIO(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
