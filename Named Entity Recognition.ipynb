{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with NLTK HMM with Words as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |#ID | #Gold Standard | #Found | #Correct | Precision | Recall | F-1 |\n",
    " |----|----------------|--------|----------|-----------|--------|-----|\n",
    "|1|3413|1441|832|0.577376821652|0.243773806036|0.342810053564|\n",
    "|2|3413|2540|1370|0.53937007874|0.401406387343|0.460272131698|\n",
    "|3|3413|2434|1308|0.537387017256|0.383240550835|0.447408927655|\n",
    "|4|3167|2418|1318|0.545078577337|0.416166719293|0.471978513876|\n",
    " 1. Training on 80% of data and 20 % dev with NLTK HMM with words as features\n",
    " 2. Training on 80% of data and 20 % dev with NLTK HMM with words as features\n",
    "     - Removing Low Frequency Words <= 1\n",
    " 3. Training on 80% of data and 20 % dev with NLTK HMM with words as features\n",
    "     - Removing Low Frequency Words <= 2\n",
    " 2. Random sample training on 80% of data and 20 % dev with NLTK HMM with words as features\n",
    "     - Removing Low Frequency Words <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:00:22.156880Z",
     "start_time": "2017-12-04T05:00:21.750473Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "from evalNER import eval\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:00:24.793040Z",
     "start_time": "2017-12-04T05:00:24.768145Z"
    }
   },
   "outputs": [],
   "source": [
    "text = open(\"./gene-trainF17.txt\").read()\n",
    "lines = [ y.strip() for y in text.split(\"\\n\\n\")]\n",
    "raw_df = pd.DataFrame(lines, columns = [\"sentence\"])\n",
    "# np.random.seed(1234)\n",
    "msk = np.random.rand(len(raw_df)) < 0.8\n",
    "train_df = raw_df[msk]\n",
    "dev_df = raw_df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:00:57.954229Z",
     "start_time": "2017-12-04T05:00:57.950996Z"
    }
   },
   "outputs": [],
   "source": [
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:00:58.856163Z",
     "start_time": "2017-12-04T05:00:58.400247Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"sentence_token\"] = df[\"sentence\"].apply(lambda x : tuple(y.split(\"\\t\") for y in x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:00.720773Z",
     "start_time": "2017-12-04T05:01:00.614108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"tags\"] = df[\"sentence_token\"].apply(lambda x : tuple(y[2] for y in x))\n",
    "df.loc[:, \"words\"] = df[\"sentence_token\"].apply(lambda x : tuple(y[1] for y in x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:01.280306Z",
     "start_time": "2017-12-04T05:01:01.225338Z"
    }
   },
   "outputs": [],
   "source": [
    "count_df = df[\"words\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:02.372966Z",
     "start_time": "2017-12-04T05:01:02.173957Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for k, v in count_df.iteritems():\n",
    "    temp_counter = Counter()\n",
    "    for w in k:\n",
    "        word_counter[w] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:02.989482Z",
     "start_time": "2017-12-04T05:01:02.974088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12373\n"
     ]
    }
   ],
   "source": [
    "V = set(k for k, v in word_counter.iteritems() if v > 1)\n",
    "print len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:04.509406Z",
     "start_time": "2017-12-04T05:01:04.427629Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"words\"] = df[\"words\"].apply(lambda x: tuple(y if y in V else \"UNK\" for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:05.503425Z",
     "start_time": "2017-12-04T05:01:05.116501Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"words_tags\"] = df[[\"words\", \"tags\"]].apply(lambda x : zip(x[0],x[1]), axis = 1)\n",
    "features = df[\"words_tags\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:07.351849Z",
     "start_time": "2017-12-04T05:01:06.847780Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:08.837346Z",
     "start_time": "2017-12-04T05:01:08.725911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = dev_df.copy()\n",
    "test_df.loc[:, \"sentence_token\"] = test_df[\"sentence\"].apply(lambda x : [y.split(\"\\t\") for y in x.split(\"\\n\")])\n",
    "test_df.loc[:, \"tags\"] = test_df[\"sentence_token\"].apply(lambda x : tuple(y[2] for y in x))\n",
    "test_df.loc[:, \"words\"] = test_df[\"sentence_token\"].apply(lambda x : tuple(y[1] if y[1] in V else \"UNK\" for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:11.397117Z",
     "start_time": "2017-12-04T05:01:09.415243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.loc[:, \"prediction\"] = test_df[\"words\"].apply(lambda x: tuple(\"\\t\".join(x) for x in tagger.tag(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T05:01:12.534507Z",
     "start_time": "2017-12-04T05:01:12.298566Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3167, ' entities in gold standard.')\n",
      "(2418, ' total entities found.')\n",
      "(1318, ' of which were correct.')\n",
      "('Precision: ', 0.5450785773366419, 'Recall: ', 0.41616671929270604, 'F1-measure: ', 0.47197851387645484)\n",
      "|3167|2418|1318|0.545078577337|0.416166719293|0.471978513876|\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[:, \"temp1\"] = test_df[\"prediction\"].apply(lambda x : tuple(str(i) + \"\\t\" + y for i,y in enumerate(x,1)))\n",
    "test_df.loc[:, \"temp1\"] = test_df[\"temp1\"].apply(lambda x : \"\\n\".join(x))\n",
    "\n",
    "predictions = \"\\n\\n\".join(test_df[\"temp1\"].tolist())\n",
    "gold_standard = \"\\n\\n\".join(test_df[\"sentence\"].tolist())\n",
    "eval(StringIO(gold_standard), StringIO(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
