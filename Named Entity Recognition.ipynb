{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with NLTK MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |#ID | #Gold Standard | #Found | #Correct | Precision | Recall | F-1 |\n",
    " |----|----------------|--------|----------|-----------|--------|-----|\n",
    " |1|3413|45|1|0.022222222222222223|0.0002929973630237328|0.000578368999421631|\n",
    " |2|3413|291|61|0.209621993127|0.0178728391444|0.0329373650108|\n",
    " |3|3413|1843|629|0.341291372762|0.184295341342|0.239345509893|\n",
    " |4|3413|1729|660|0.381723539618|0.193378259596|0.256709451575|\n",
    " |5|3413|2830|833|0.294346289753|0.244066803399|0.266858881948|\n",
    " |6|3413|1950|843|0.432307692308|0.246996777029|0.314376281932|\n",
    "|7|3413|2090|1009|0.482775119617|0.295634339291|0.366709067781|\n",
    "|8|3413|2217|1118|0.504285069914|0.327571051861|0.397158081705|\n",
    "|9|3413|2396|1338|0.558430717863|0.392030471726|0.460664486142|\n",
    "|10|3413|2355|1319|0.56008492569|0.386463521828|0.457350901526|\n",
    "|11|3413|2479|1432|0.577652279145|0.41957222385|0.486082824168|\n",
    "|12|3413|2601|1556|0.598231449443|0.455903896865|0.517459261723|\n",
    "|13|3372|2570|1508|0.586770428016|0.447212336892|0.507573207674|\n",
    "|14|3413|2641|1566|0.592957213177|0.458833870495|0.517343904856|\n",
    "|15|3263|2445|1420|0.580777096115|0.435182347533|0.497547302032|\n",
    "|16|3413|2698|1765|0.65418828762|0.517140345737|0.577646866307|\n",
    "|17|3413|2854|2075|0.72704975473|0.607969528274|0.662198819212|\n",
    "|18|3413|2803|1886|0.672850517303|0.552593026663|0.606821106821|\n",
    "|19|3371|2750|1812|0.658909090909|0.537525956689|0.592060120895|\n",
    "|20|3188|2496|1602|0.641826923077|0.502509410289|0.563687543983|\n",
    "|21|3395|2615|1653|0.632122370937|0.486892488954|0.550083194676|\n",
    "|22|3321|2507|1540|0.614280015955|0.463715748269|0.528483184626|\n",
    "|23|3448|2574|1600|0.621600621601|0.46403712297|0.531384921953|\n",
    "||3388|2856|1976|0.6918767507|0.583234946871|0.632927610506|\n",
    "Training on 80% of data and 20% dev with NLTK MaxEnt\n",
    "\n",
    " 1. Feature : POS, Word Position\n",
    " 2. Feature : POS, Word Position, Word\n",
    " 3. Feature : POS, Word Position, Word, word Shape\n",
    " 4. Feature : POS, Previous POS, Word Position, Word, word Shape\n",
    " 5. Feature : current pos, previous pos, word position, current word, previous word, word Shape\n",
    " 6. Feature : current pos, previous pos, word position, current word, previous word, current word shape, previous word shape\n",
    " 7. Feature : {current, previous, next} pos, word position, {current, previous, next} word, {current, previous, next} word shape\n",
    " 8. Feature : {current, previous, next} pos, word position, {current, previous, next} word, {current, previous, next} word shape, current word len\n",
    " 9. Feature : {current, previous, next} {pos, word, word shape}, current word len, word position, suffix3, prefix3\n",
    " 10. Feature : {current, previous, next} {pos, word, word_shape, lemma},  word position, current word len, suffix3, prefix3\n",
    " 11. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len},  word position, suffix3, prefix3\n",
    " 12. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position\n",
    " 13. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position\n",
    "     - Random Data Set\n",
    " 14. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position, currrent pos + previous pos, current pos + next pos\n",
    " \n",
    " 15. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position, currrent pos + previous pos, current pos + next pos\n",
    "     - Random Data Set\n",
    " 16. Feature : {current, previous, next} {pos, word, word_shape, lemma, word len, suffix3, prefix3},  word position, currrent pos + previous pos, current pos + next pos, prev_tag\n",
    " 17. Same as 16. but different data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:45.119941Z",
     "start_time": "2017-12-06T05:04:45.095544Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:45.579090Z",
     "start_time": "2017-12-06T05:04:45.121294Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "from evalNER import eval\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk import MaxentClassifier\n",
    "from nltk.chunk import named_entity\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:45.602608Z",
     "start_time": "2017-12-06T05:04:45.580517Z"
    }
   },
   "outputs": [],
   "source": [
    "text = open(\"./gene-trainF17.txt\").read()\n",
    "lines = [ y.strip() for y in text.split(\"\\n\\n\")]\n",
    "raw_df = pd.DataFrame(lines, columns = [\"sentence\"])\n",
    "# msk = np.random.rand(len(raw_df)) < 0.8\n",
    "# train_df = raw_df[msk]\n",
    "# dev_df = raw_df[~msk]\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:45.614684Z",
     "start_time": "2017-12-06T05:04:45.603773Z"
    }
   },
   "outputs": [],
   "source": [
    "df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:46.031082Z",
     "start_time": "2017-12-06T05:04:45.616053Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"sentence_token\"] = df[\"sentence\"].apply(lambda x : tuple(y.split(\"\\t\") for y in x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:46.122249Z",
     "start_time": "2017-12-06T05:04:46.032567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"tags\"] = df[\"sentence_token\"].apply(lambda x : tuple(y[2] for y in x))\n",
    "df.loc[:, \"words\"] = df[\"sentence_token\"].apply(lambda x : tuple(y[1] for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:04:46.313715Z",
     "start_time": "2017-12-06T05:04:46.123769Z"
    }
   },
   "outputs": [],
   "source": [
    "df_count = df[\"words\"].value_counts()\n",
    "word_counter = Counter()\n",
    "\n",
    "for k in df_count.keys():\n",
    "    for w in k:\n",
    "        word_counter[w] += 1\n",
    "\n",
    "V = set(k for k,v in word_counter.iteritems() if v > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:05:00.581587Z",
     "start_time": "2017-12-06T05:04:46.315016Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"words_\"] = df[\"sentence_token\"].apply(lambda x : tuple(y[1] for y in x))\n",
    "df.loc[:, \"pos\"] = df[\"words_\"].apply(lambda x : tuple( y[1] for y in nltk.pos_tag(x)))\n",
    "df.loc[:, \"words\"] = df[\"words_\"].apply(lambda x : tuple(y for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:05:00.668886Z",
     "start_time": "2017-12-06T05:05:00.583239Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(num, word_tuple, pos_tuple, prev_tag):\n",
    "    feature = {}\n",
    "    next_elem = lambda itr, idx: itr[idx + 1] if idx < len(itr) - 1 else None\n",
    "    prev_elem = lambda itr, idx: itr[idx - 1] if idx > 0 else None\n",
    "    \n",
    "#     word_tuple = map(lambda x : x if x in V else \"UNK\", word_tuple)\n",
    "    prev_word = prev_elem(word_tuple, num)\n",
    "    word = word_tuple[num]\n",
    "    next_word = next_elem(word_tuple, num)\n",
    "    \n",
    "    prev_pos = prev_elem(pos_tuple, num)\n",
    "    pos = pos_tuple[num]\n",
    "    next_pos = next_elem(pos_tuple, num)\n",
    "    \n",
    "    feature[\"index\"] = num\n",
    "    \n",
    "    prefix = lambda x,y : x[:y].lower() if x else None\n",
    "    \n",
    "    feature[\"prev_prefix3\"] = prefix(prev_word,3)\n",
    "    feature[\"curr_prefix3\"] = prefix(word,3)\n",
    "    feature[\"next_prefix3\"] = prefix(next_word,3)\n",
    "    \n",
    "    suffix = lambda x, y : x[-y:].lower() if x else None\n",
    "    \n",
    "    feature[\"prev_suffix3\"] = suffix(prev_word,3)\n",
    "    feature[\"curr_suffix3\"] = suffix(word,3)\n",
    "    feature[\"next_suffix3\"] = suffix(next_word,3)\n",
    "    \n",
    "    feature[\"prev_len\"] = len(prev_word) if prev_word else 0\n",
    "    feature[\"curr_len\"] = len(word)\n",
    "    feature[\"next_len\"] = len(next_word) if next_word else 0\n",
    "    \n",
    "    feature[\"unknown_word\"] = word if word in V else \"#UNK#\"\n",
    "    \n",
    "    feature[\"prev_word\"] = prev_word\n",
    "    feature[\"curr_word\"] = word\n",
    "    feature[\"next_word\"] = next_word\n",
    "    \n",
    "    feature[\"prev_pos\"] = prev_pos\n",
    "    feature[\"curr_pos\"] = pos\n",
    "    feature[\"next_pos\"] = next_pos\n",
    "    \n",
    "    \n",
    "    feature[\"curr_shape\"] = named_entity.shape(word)\n",
    "    feature[\"prev_shape\"] = named_entity.shape(prev_word) if prev_word else None\n",
    "    feature[\"next_shape\"] = named_entity.shape(next_word) if next_word else None\n",
    "    \n",
    "    \n",
    "    feature[\"curr_lemma\"] = stemmer.stem(word)\n",
    "    feature[\"prev_lemma\"] = stemmer.stem(prev_word) if prev_word else None\n",
    "    feature[\"next_lemma\"] = stemmer.stem(next_word) if next_word else None\n",
    "\n",
    "    feature[\"prev_prefix3+curr_prefix3\"] = \"%s+%s\" % (feature[\"prev_prefix3\"], feature[\"curr_prefix3\"])\n",
    "    feature[\"curr_prefix3+next_suffix3\"] = \"%s+%s\" % (feature[\"curr_prefix3\"], feature[\"next_suffix3\"])\n",
    "    feature[\"prev_prefix3+curr_prefix3+next_suffix3\"] = \"%s+%s+%s\" % (feature[\"prev_prefix3\"], feature[\"curr_prefix3\"], feature[\"next_suffix3\"])\n",
    "    \n",
    "    feature[\"prev_suffix3+curr_suffix3\"] = \"%s+%s\" % (feature[\"prev_suffix3\"], feature[\"curr_suffix3\"])\n",
    "    feature[\"curr_suffix3+next_suffix3\"] = \"%s+%s\" % (feature[\"curr_suffix3\"], feature[\"next_suffix3\"])\n",
    "    feature[\"prev_suffix3+curr_suffix3+next_suffix3\"] = \"%s+%s+%s\" % (feature[\"prev_suffix3\"], feature[\"curr_suffix3\"], feature[\"next_suffix3\"])\n",
    "                                     \n",
    "    feature[\"curr_pos+next_pos\"] = \"%s+%s\" % (pos, next_pos)\n",
    "    feature[\"prev_pos+curr_pos\"] = \"%s+%s\" % (prev_pos, pos)\n",
    "    feature[\"prev_pos+curr_pos+next_pos\"] = \"%s+%s+%s\" %(prev_pos, pos, next_pos)\n",
    "    \n",
    "    feature[\"prev_shape+curr_shape\"] = \"%s+%s\" % (feature[\"prev_shape\"], feature[\"curr_shape\"])\n",
    "    feature[\"curr_shape+next_shape\"] = \"%s+%s\" % (feature[\"curr_shape\"], feature[\"next_shape\"])\n",
    "    feature[\"prev_shape+curr_shape+next_shape\"] = \"%s+%s+%s\" % (feature[\"prev_shape\"], feature[\"curr_shape\"], feature[\"next_shape\"])\n",
    "    \n",
    "    feature[\"prev_lemma+curr_lemma\"] = \"%s+%s\" % (feature[\"prev_lemma\"], feature[\"curr_lemma\"])\n",
    "    feature[\"curr_lemma+next_lemma\"] = \"%s+%s\" % (feature[\"curr_lemma\"], feature[\"next_lemma\"])\n",
    "    feature[\"prev_lemma+curr_lemma+next_lemma\"] = \"%s+%s+%s\" % (feature[\"prev_lemma\"], feature[\"curr_lemma\"], feature[\"next_lemma\"])\n",
    "    \n",
    "    feature[\"curr_word+next_word\"] = \"%s+%s\" % (word, next_word)\n",
    "    feature[\"prev_word+curr_word\"] = \"%s+%s\" % (prev_word, word)\n",
    "    feature[\"prev_word+curr_word+next_word\"] = \"%s+%s+%s\" %(prev_word, word, next_word)\n",
    "    \n",
    "    feature[\"prev_tag1\"]  = prev_tag[0]\n",
    "    feature[\"prev_tag2\"]  = prev_tag[1]\n",
    "    feature[\"prev_tag3\"]  = prev_tag[2]\n",
    "    return feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:05:17.865807Z",
     "start_time": "2017-12-06T05:05:00.670186Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "for index, tuples in df[[\"words\", \"pos\", \"tags\"]].iterrows():\n",
    "    word_tuple, pos_tuple, tag_tuple = tuples\n",
    "    word_num = 0\n",
    "    prev_tag = prev_tag1 = prev_tag2 = None\n",
    "    for word_num in range(len(word_tuple)):\n",
    "        feature = (extract_features(word_num, word_tuple, pos_tuple, [prev_tag2, prev_tag1, prev_tag]), tag_tuple[word_num])\n",
    "        features.append(feature)\n",
    "        prev_tag = tag_tuple[word_num]\n",
    "        prev_tag1 = prev_tag\n",
    "        prev_tag2 = prev_tag1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:25.448594Z",
     "start_time": "2017-12-06T05:05:17.868636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception RuntimeError: 'generator ignored GeneratorExit' in <generator object find_file_iter at 0x7f6bd717bc30> ignored\n"
     ]
    }
   ],
   "source": [
    "memm_classifier = MaxentClassifier.train(features, \"megam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:26.083940Z",
     "start_time": "2017-12-06T05:22:25.451025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = open(\"./goldoutput.txt\").read()\n",
    "lines = [ y.strip() for y in text.split(\"\\n\\n\")]\n",
    "test_df = pd.DataFrame(lines, columns = [\"sentence\"])\n",
    "# test_df = dev_df.copy()\n",
    "test_df.loc[:, \"sentence_token\"] = test_df[\"sentence\"].apply(lambda x : tuple(y.split(\"\\t\") for y in x.split(\"\\n\")))\n",
    "test_df.loc[:, \"words_\"] = test_df[\"sentence_token\"].apply(lambda x : tuple(y[1] for y in x))\n",
    "test_df.loc[:, \"pos\"] = test_df[\"words_\"].apply(lambda x :  tuple(x[1] for x in nltk.pos_tag(x)))\n",
    "test_df.loc[:, \"words\"] = test_df[\"words_\"].apply(lambda x : tuple(y for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:28.553686Z",
     "start_time": "2017-12-06T05:22:26.085035Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for _, tuples in test_df[[\"pos\", \"words\"]].iterrows():\n",
    "    pos_tuple, word_tuple = tuples\n",
    "    prev_tag = prev_tag1 = prev_tag2 = None\n",
    "    prediction = []\n",
    "    for word_num in range(len(word_tuple)):\n",
    "        feature = extract_features(word_num, word_tuple, pos_tuple, [prev_tag2, prev_tag1, prev_tag])\n",
    "        prev_tag = predict = memm_classifier.classify(feature)\n",
    "        prev_tag1 = prev_tag\n",
    "        prev_tag2 = prev_tag1\n",
    "        prediction.append(predict)\n",
    "    predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:28.565676Z",
     "start_time": "2017-12-06T05:22:28.554953Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[\"prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:28.600273Z",
     "start_time": "2017-12-06T05:22:28.566719Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.loc[:, \"temp1\"] = test_df[[\"prediction\", \"words_\"]].apply(lambda x : [str(i) + \"\\t\" + \"\\t\".join(y) for i,y in enumerate(zip( x[\"words_\"], x[\"prediction\"]), 1)], axis = 1)\n",
    "test_df.loc[:, \"temp1\"] = test_df[\"temp1\"].apply(lambda x : \"\\n\".join(x))\n",
    "\n",
    "predictions = \"\\n\\n\".join(test_df[\"temp1\"].tolist())\n",
    "# gold_standard = \"\\n\\n\".join(test_df[\"sentence\"].tolist())\n",
    "# eval(StringIO(gold_standard), StringIO(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T05:22:28.614561Z",
     "start_time": "2017-12-06T05:22:28.601682Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"sharma-hasil-assgn4-out.txt\", \"w\") as f:\n",
    "    f.write(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
